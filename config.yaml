# WitsV3 Configuration File
# Streamlined for LLM Wrapper Architecture (CLI First)

# General Settings
project_name: "WitsV3"
version: "3.0.0"
logging_level: "INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL
debug_mode: false

# LLM Configuration (Primarily Ollama) - Enhanced with Model Specialization
llm_interface:
  default_provider: "ollama" # ollama, openai, etc.
  timeout_seconds: 120
  streaming_enabled: true

ollama_settings:
  url: "http://localhost:11434"
  default_model: "llama3" # Default model for general tasks

  # Specialized model assignments based on agent capabilities
  control_center_model: "hf.co/google/gemma-3-4b-it-qat-q4_0-gguf" # Better reasoning for routing
  orchestrator_model: "llama3" # Balanced reasoning and planning
  coding_agent_model: "deepseek-coder-v2:16b-lite-instruct-q4_K_M" # Advanced coding tasks
  book_writing_model: "hf.co/google/gemma-3-4b-it-qat-q4_0-gguf" # Creative writing and narrative
  self_repair_model: "llama3" # System monitoring and repair
  neural_reasoning_model: "llama3" # Neural web reasoning and analysis

  embedding_model: "nomic-embed-text" # Use dedicated embedding model
  request_timeout: 120

# Agent Configuration
agents:
  default_temperature: 0.7
  max_iterations: 15 # Max iterations for ReAct loop in Orchestrator

  # Agent-specific configurations
  control_center_agent:
    model: "hf.co/google/gemma-3-4b-it-qat-q4_0-gguf" # Was openhermes, changed to Gemma
    temperature: 0.3
    max_tokens: 2048
    enable_neural_routing: true

  orchestrator_agent:
    model: "llama3"
    temperature: 0.7
    max_tokens: 4096
    enable_neural_reasoning: true
    reasoning_depth: 3

  book_writing_agent:
    model: "hf.co/google/gemma-3-4b-it-qat-q4_0-gguf"
    temperature: 0.8
    max_tokens: 16384
    enable_narrative_intelligence: true
    story_structure_patterns: ["hero_journey", "three_act", "spiral"]

  coding_agent:
    model: "deepseek-coder-v2:16b-lite-instruct-q4_K_M"
    temperature: 0.2
    max_tokens: 8192
    enable_code_intelligence: true
    supported_languages: ["python", "javascript", "typescript", "rust", "go"]

  self_repair_agent:
    model: "llama3"
    temperature: 0.1
    max_tokens: 4096
    enable_system_monitoring: true
    health_check_interval: 300

# Memory Management - Enhanced with Neural Web Support
memory_manager:
  # Enhanced backend options including neural web
  backend: "neural" # 'basic' (json file), 'faiss_cpu', 'faiss_gpu', 'neural'
  memory_file_path: "data/wits_memory.json" # For basic backend
  faiss_index_path: "data/wits_faiss_index.bin" # For FAISS backends
  neural_web_path: "data/neural_web.json" # For neural web persistence
  vector_dim: 384 # Was 4096, changed to match all-MiniLM-L6-v2
  max_results_per_search: 5
  pruning_interval_seconds: 3600 # How often to check for pruning
  max_memory_segments: 10000 # Max segments before pruning oldest

  # Neural Web Configuration
  neural_web_settings:
    activation_threshold: 0.3
    decay_rate: 0.1
    auto_connect: true
    reasoning_patterns: ["modus_ponens", "analogy", "chain", "contradiction"]
    max_concept_connections: 50
    connection_strength_threshold: 0.2

# Tool System
tool_system:
  enable_mcp_tools: true
  mcp_tool_definitions_path: "data/mcp_tools.json"
  # Langchain integration settings (if applicable later)
  # langchain_bridge_enabled: false

# CLI Settings
cli:
  show_thoughts: true
  show_tool_calls: true
# Add other necessary configurations as WitsV3 evolves
