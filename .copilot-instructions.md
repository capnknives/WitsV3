# WitsV3 GitHub Copilot Instructions

# Personalized coding rules and preferences for the WitsV3 LLM orchestration system

## ðŸŽ¯ Project Overview & Context

WitsV3 is an advanced LLM orchestration system with:

- **Architecture**: CLI-first, ReAct pattern, tool registry, model reliability
- **Core Philosophy**: Async-first, streaming responses, robust error handling
- **Recent Major Features**: Enhanced tool validation, streaming error context, model reliability system with automatic fallbacks
- **Development Approach**: Phase-based implementation with comprehensive testing

**Always check TASK.md first** - it tracks progress and contains the current phase priorities.

## ðŸ Python Code Style & Standards

### Core Requirements

- **Python 3.10+** with comprehensive type hints
- **Fully async architecture** - all agent/tool methods must be async
- **PEP8 compliance** with black formatting
- **Pydantic models** for all data validation
- **Comprehensive error handling** with try/catch blocks

### Import Conventions

```python
# Standard library first
import asyncio
from typing import Dict, List, Optional, AsyncGenerator

# Third-party imports
import httpx
from pydantic import BaseModel

# Project imports - relative within packages
from .schemas import StreamData, ToolCall
from ..agents.base_agent import BaseAgent
```

### Type Hints Pattern

```python
async def process_data(
    self,
    input_data: str,
    model: Optional[str] = None,
    temperature: Optional[float] = None
) -> AsyncGenerator[StreamData, None]:
    """Always use comprehensive type hints."""
```

## ðŸ—ï¸ Architecture & Structure Rules

### File Organization

- **Max 500 lines per file** - modularize when approaching this limit
- **Strict directory structure**:
  - `core/` - Config, LLM interface, schemas, memory, reliability
  - `agents/` - All agent implementations extending BaseAgent
  - `tools/` - Tool implementations extending BaseTool
  - `tests/` - Mirror the main structure for test organization

### Class Design Patterns

```python
class YourAgent(BaseAgent):
    """All agents must extend BaseAgent."""

    def __init__(self, agent_name: str, config: WitsV3Config, ...):
        super().__init__(agent_name, config, ...)
        self.logger = logging.getLogger(f"WitsV3.{self.__class__.__name__}")

    async def run(self, user_input: str, **kwargs) -> AsyncGenerator[StreamData, None]:
        """Main agent method - always async generator."""
```

### Tool Pattern

```python
class YourTool(BaseTool):
    """All tools must extend BaseTool."""

    def __init__(self):
        super().__init__(
            name="tool_name",
            description="Clear description for LLM"
        )

    async def execute(self, **kwargs) -> Any:
        """Tool execution - always async."""
```

## ðŸ”§ WitsV3-Specific Patterns

### StreamData Usage

```python
# Use specific stream types for clear communication
yield self.stream_thinking("Analyzing the request...")
yield self.stream_action("Calling the calculator tool")
yield self.stream_observation("Tool returned: 42")
yield self.stream_result("The answer is 42")

# For errors, use enhanced error context
yield StreamData(
    type="error",
    content="Operation failed",
    source=self.agent_name,
    error_code="CALCULATION_ERROR",
    error_category="tool_execution",
    severity="medium",
    trace_id=str(uuid.uuid4()),
    suggested_actions=["Check input format", "Retry with different parameters"]
)
```

### Tool Registry Integration

```python
# Tools must provide LLM descriptions
def get_schema(self) -> Dict[str, Any]:
    return {
        "name": self.name,
        "description": self.description,
        "parameters": {
            "type": "object",
            "properties": {
                "param": {"type": "string", "description": "Parameter description"}
            },
            "required": ["param"]
        }
    }
```

### Enhanced Validation Pattern

```python
# Use enhanced validation for all tools
async def execute(self, **kwargs) -> Any:
    # Validate using enhanced schemas
    validation_result = await self.validate_arguments(kwargs)
    if not validation_result.is_valid:
        raise ValueError(f"Validation failed: {validation_result.errors}")

    # Your tool logic here
    return result
```

### Model Reliability Integration

```python
# Use enhanced LLM interface for automatic fallbacks
from core.enhanced_llm_interface import get_enhanced_llm_interface

# This automatically handles model failures and fallbacks
llm_interface = get_enhanced_llm_interface(config)
response = await llm_interface.generate_text(prompt, model="preferred_model")
```

## ðŸ§ª Testing Standards

### Test Structure

```python
import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock

class TestYourFeature:
    """Group related tests in classes."""

    @pytest.mark.asyncio
    async def test_happy_path(self):
        """Test normal operation."""

    @pytest.mark.asyncio
    async def test_error_handling(self):
        """Test error scenarios."""

    @pytest.mark.asyncio
    async def test_edge_cases(self):
        """Test boundary conditions."""
```

### Enhanced Validation Test Pattern

```python
@pytest.mark.asyncio
async def test_tool_validation(tool_instance):
    """Test enhanced tool validation."""
    # Test valid arguments
    result = await tool_instance.validate_arguments({"param": "valid_value"})
    assert result.valid
    assert len(result.errors) == 0

    # Test invalid arguments
    result = await tool_instance.validate_arguments({"param": ""})
    assert not result.valid
    assert "minimum length" in str(result.errors)

    # Test missing required parameters
    result = await tool_instance.validate_arguments({})
    assert not result.valid
    assert "Missing required parameters" in str(result.errors)
```

### Streaming Error Context Test Pattern

```python
@pytest.mark.asyncio
async def test_error_context_streaming():
    """Test streaming with enhanced error context."""
    error_stream = StreamData(
        type="error",
        content="Test error",
        error_code="TEST_ERROR",
        error_category="validation",
        severity="medium",
        trace_id="test-trace-123",
        suggested_actions=["Check input", "Retry"]
    )

    assert error_stream.is_error()
    assert error_stream.error_category == "validation"
    assert "Check input" in error_stream.suggested_actions
```

### Required Test Coverage

- **Happy path**: Normal operation works
- **Error handling**: Graceful failure handling
- **Edge cases**: Boundary conditions and unusual inputs
- **Async patterns**: Proper async/await usage
- **Mocking**: External services (Ollama, file system)
- **Enhanced validation**: Parameter validation and error messages
- **Error context**: Trace IDs, error categorization, suggested actions

### Test File Naming

- Mirror the main structure: `tests/core/test_model_reliability.py`
- Use descriptive test names: `test_model_fallback_on_failure`
- Include validation tests: `test_enhanced_validation.py`
- Include streaming tests: `test_enhanced_streaming.py`

## ðŸ“ Documentation Requirements

### Docstring Format

```python
async def complex_method(
    self,
    input_data: str,
    options: Optional[Dict[str, Any]] = None
) -> AsyncGenerator[StreamData, None]:
    """
    Process complex data with streaming results.

    This method demonstrates the preferred docstring format with
    clear parameter descriptions and return value documentation.

    Args:
        input_data: The data to process
        options: Optional processing parameters

    Yields:
        StreamData: Streaming response data

    Raises:
        ValueError: If input_data is invalid
        ConnectionError: If LLM service is unavailable

    Example:
        async for stream_data in agent.complex_method("test"):
            print(stream_data.content)
    """
```

### Code Comments

```python
# Use comments to explain WHY, not WHAT
# Reason: Model reliability requires fallback strategy
if not primary_response:
    # Fallback to secondary model for resilience
    response = await self._try_fallback_model(prompt)
```

## âš ï¸ Critical WitsV3 Rules

### Must Always Do

1. **Preserve async patterns** - Never use synchronous I/O
2. **Use streaming responses** - Agents must yield StreamData
3. **Handle failures gracefully** - Robust error handling everywhere
4. **Update TASK.md** - Track progress and completion dates
5. **Test with real Ollama** - Many features require active connection
6. **Validate all inputs** - Use Pydantic and enhanced validation schemas
7. **Use model reliability** - Leverage automatic fallback system
8. **Include error context** - Always provide trace IDs and suggested actions
9. **Follow Phase guidelines** - Phase 1 completed, focus on Phase 2 Neural Web

### Never Do

1. **Break backward compatibility** without migration plan
2. **Hardcode configuration** - Always use config.yaml
3. **Create circular imports** between packages
4. **Skip error handling** - Every external call needs try/catch
5. **Forget logging** - Use self.logger for debugging
6. **Ignore memory management** - Be mindful of large data structures
7. **Skip validation** - All tool arguments must be validated
8. **Use synchronous I/O** - Everything must be async
9. **Ignore test coverage** - Write comprehensive tests for all new features

### Pattern Enforcement

- **Tool development**: Always extend BaseTool and use enhanced validation
- **Agent development**: Always extend BaseAgent and use streaming responses
- **Error handling**: Always include context, trace IDs, and actionable suggestions
- **Model reliability**: Use ReliableOllamaInterface for all LLM interactions

## ðŸ”„ Development Workflow

### Before Starting

1. Read TASK.md to understand current priorities
2. Check if the task exists, add it if not
3. Understand the phase context (**Phase 1: COMPLETED** âœ…, **Phase 2: Active**)
4. Review recent enhancement patterns from Phase 1 success

### During Development

1. Write tests first for TDD approach
2. Use existing patterns and conventions
3. Add comprehensive error handling
4. Include logging for debugging
5. Update docstrings and comments

### After Completion

1. Run test suite: `pytest -v --asyncio-mode=auto`
2. Update TASK.md with completion date
3. Add documentation if needed
4. Commit with descriptive message

## ðŸŽ¨ Coding Preferences (Based on Our Work Together)

### Error Handling Style

```python
# Prefer comprehensive error context
try:
    result = await risky_operation()
except SpecificError as e:
    self.logger.error(f"Operation failed: {e}")
    # Provide actionable error information
    yield StreamData(
        type="error",
        content=f"Operation failed: {str(e)}",
        error_category="operation_failure",
        suggested_actions=["Check configuration", "Retry"]
    )
    return
```

### Configuration Access

```python
# Always use config for settings
timeout = self.config.ollama_settings.request_timeout
model = self.config.ollama_settings.default_model
```

### Performance Considerations

```python
# Use caching where appropriate
@lru_cache(maxsize=128)
def expensive_computation(self, input_data: str) -> str:
    """Cache expensive operations."""

# Prefer async context managers
async with httpx.AsyncClient() as client:
    response = await client.post(url, json=data)
```

## ðŸš€ Recent Enhancements to Leverage

1. **Enhanced Tool Validation**: Use ToolParameter and ToolSchema for robust validation with type checking, range validation, pattern matching, and enum validation
2. **Streaming Error Context**: Include error_code, severity, trace_id, correlation_id, suggested_actions in error responses
3. **Model Reliability System**: Automatic fallbacks and health monitoring with quarantine and recovery mechanisms
4. **Enhanced LLM Interface**: Use ReliableOllamaInterface for better resilience with automatic model selection

### Key Implementation Patterns from Phase 1 Success

```python
# Enhanced Tool Validation Pattern
def get_enhanced_schema(self) -> ToolSchema:
    return ToolSchema(
        name=self.name,
        description=self.description,
        parameters=[
            ToolParameter(
                name="param",
                type="string",
                required=True,
                min_length=1,
                max_length=100,
                pattern=r"^[a-zA-Z0-9_]+$"
            )
        ]
    )

# Error Context Pattern with Tracing
yield StreamData(
    type="error",
    content="Operation failed",
    source=self.agent_name,
    error_code="VALIDATION_ERROR",
    error_category="tool_execution",
    severity="medium",
    trace_id=str(uuid.uuid4()),
    correlation_id=context.get("correlation_id"),
    suggested_actions=["Check parameter format", "Verify input data"]
)

# Model Reliability Pattern
from core.enhanced_llm_interface import get_enhanced_llm_interface
llm_interface = get_enhanced_llm_interface(config)
response = await llm_interface.generate_text(prompt, model="preferred_model")
```

## ðŸ“‹ Current Phase Priorities

Based on TASK.md, focus areas are:

- **Phase 1**: âœ… **COMPLETED** (Tool validation, error context, model reliability) - ALL TESTS PASSING
  - âœ… Enhanced tool validation with comprehensive test suite (3/3 tests passing)
  - âœ… Streaming error context with trace correlation (2/2 tests passing)
  - âœ… Model reliability system with automatic fallbacks (3/3 tests passing)
- **Phase 2**: Neural Web integrations and cross-domain learning (CURRENT FOCUS)
- **Phase 3**: Core enhancements (Adaptive LLM, CLI improvements)
- **Phase 4**: New features (Web UI, Langchain integration)

### Phase 2 Development Guidelines

When working on Neural Web integrations:

- Leverage existing cross-domain learning capabilities in `core/neural_web/cross_domain_learning.py`
- Use knowledge graph patterns from `core/neural_web/knowledge_graph.py`
- Integrate with working memory system in `core/neural_web/working_memory.py`
- Follow concept activation propagation patterns from established NeuralWeb classes
- Ensure backward compatibility with existing agent framework
- Build upon the solid foundation of Phase 1 enhancements

When suggesting code, prioritize Phase 2 patterns and Neural Web integration needs while leveraging the robust Phase 1 infrastructure.
